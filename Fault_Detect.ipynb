{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashraf-Saleh/Fault-Detect/blob/Master/Fault_Detect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaIp7ke4k58l",
        "outputId": "96b88fe0-6931-4da4-bc84-40f1f5967cab"
      },
      "source": [
        "!pip install pydot\n",
        "!apt install graphviz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot) (3.0.9)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "graphviz is already the newest version (2.40.1-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9PFzkDY9Spa"
      },
      "source": [
        "import numpy\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import keras \n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense, Flatten, Dropout, Activation, BatchNormalization,Concatenate\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import model_from_json\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import random, os\n",
        "import multiprocessing as mp\n",
        "from queue import Empty\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4l8tGQJ9v91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f4ec73-3955-4796-9f0e-169a795514d4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpO0tJE1908T"
      },
      "source": [
        "Data_X = numpy.load(\"/content/drive/My Drive/Colab_Data/Fault_Detect/Train_X_Sample.npy\",None,allow_pickle=True)\n",
        "Data_Y = numpy.load(\"/content/drive/My Drive/Colab_Data/Fault_Detect/Train_Y_Sample.npy\",None,allow_pickle=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyuU4RWd95V2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac055883-cef0-4d2f-bc44-f28e62c6c624"
      },
      "source": [
        "#Train_X = Data_X\n",
        "#Train_Y = Data_Y\n",
        "Train_X = Data_X[:int(len(Data_X)*0.7)]\n",
        "Train_Y = Data_Y[:int(len(Data_Y)*0.7)]\n",
        "#Vald_X = Data_X[int(len(Data_X)*0.4):int(len(Data_X)*0.7)]\n",
        "#Vald_Y = Data_Y[int(len(Data_Y)*0.4):int(len(Data_Y)*0.7)]\n",
        "Test_X = Data_X[int(len(Data_X)*0.7):]\n",
        "Test_Y = Data_Y[int(len(Data_Y)*0.7):]\n",
        " \n",
        "print(Data_X.shape)\n",
        "print(Train_X.shape)\n",
        "#print(Vald_X.shape)\n",
        "print(Test_X.shape)\n",
        "\n",
        "print(Data_Y.shape)\n",
        "print(Train_Y.shape)\n",
        "#print(Vald_Y.shape)\n",
        "print(Test_Y.shape)\n",
        "\n",
        "print(Data_X.shape)\n",
        "print(Data_Y.shape)\n",
        "print(Data_X[:10])\n",
        "print(Data_Y[:10].astype('int'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(22950, 5, 5000, 1)\n",
            "(16064, 5, 5000, 1)\n",
            "(6886, 5, 5000, 1)\n",
            "(22950, 9)\n",
            "(16064, 9)\n",
            "(6886, 9)\n",
            "(22950, 5, 5000, 1)\n",
            "(22950, 9)\n",
            "[[[[ 0.13094077]\n",
            "   [ 0.13426788]\n",
            "   [ 0.13725989]\n",
            "   ...\n",
            "   [ 0.29248513]\n",
            "   [ 0.29248513]\n",
            "   [ 0.29248513]]\n",
            "\n",
            "  [[ 0.1185824 ]\n",
            "   [ 0.12181842]\n",
            "   [ 0.12472851]\n",
            "   ...\n",
            "   [ 0.2930481 ]\n",
            "   [ 0.2930481 ]\n",
            "   [ 0.2930481 ]]\n",
            "\n",
            "  [[ 0.06765384]\n",
            "   [ 0.06765384]\n",
            "   [ 0.06765384]\n",
            "   ...\n",
            "   [ 0.0653845 ]\n",
            "   [ 0.0653845 ]\n",
            "   [ 0.0653845 ]]\n",
            "\n",
            "  [[ 0.10997413]\n",
            "   [ 0.10997413]\n",
            "   [ 0.10997413]\n",
            "   ...\n",
            "   [ 0.05445085]\n",
            "   [ 0.05445085]\n",
            "   [ 0.05445085]]\n",
            "\n",
            "  [[ 0.31572279]\n",
            "   [ 0.30807951]\n",
            "   [ 0.29945085]\n",
            "   ...\n",
            "   [ 0.20175829]\n",
            "   [ 0.20368338]\n",
            "   [ 0.20553999]]]\n",
            "\n",
            "\n",
            " [[[ 0.09996753]\n",
            "   [ 0.09996753]\n",
            "   [ 0.09996753]\n",
            "   ...\n",
            "   [ 0.1853474 ]\n",
            "   [ 0.1897277 ]\n",
            "   [ 0.19018248]]\n",
            "\n",
            "  [[ 0.09835145]\n",
            "   [ 0.09877051]\n",
            "   [ 0.10244886]\n",
            "   ...\n",
            "   [ 0.22707055]\n",
            "   [ 0.22707055]\n",
            "   [ 0.22707055]]\n",
            "\n",
            "  [[ 0.12270904]\n",
            "   [ 0.12426921]\n",
            "   [ 0.12554571]\n",
            "   ...\n",
            "   [ 0.13322839]\n",
            "   [ 0.13594687]\n",
            "   [ 0.13816893]]\n",
            "\n",
            "  [[ 0.1363514 ]\n",
            "   [ 0.13785938]\n",
            "   [ 0.13860101]\n",
            "   ...\n",
            "   [ 0.14512734]\n",
            "   [ 0.14668476]\n",
            "   [ 0.14883549]]\n",
            "\n",
            "  [[ 0.00789118]\n",
            "   [-0.00136142]\n",
            "   [-0.00696548]\n",
            "   ...\n",
            "   [ 0.04603914]\n",
            "   [ 0.03142977]\n",
            "   [ 0.01752043]]]\n",
            "\n",
            "\n",
            " [[[ 0.06353688]\n",
            "   [ 0.06353688]\n",
            "   [ 0.06353688]\n",
            "   ...\n",
            "   [-0.67472249]\n",
            "   [-0.67472249]\n",
            "   [-0.67472249]]\n",
            "\n",
            "  [[ 0.09911972]\n",
            "   [ 0.09911972]\n",
            "   [ 0.09911972]\n",
            "   ...\n",
            "   [-0.5871726 ]\n",
            "   [-0.5871726 ]\n",
            "   [-0.58719588]]\n",
            "\n",
            "  [[ 0.14058011]\n",
            "   [ 0.13977638]\n",
            "   [ 0.13847624]\n",
            "   ...\n",
            "   [ 0.0853358 ]\n",
            "   [ 0.08519397]\n",
            "   [ 0.08495758]]\n",
            "\n",
            "  [[ 0.13983706]\n",
            "   [ 0.13899654]\n",
            "   [ 0.13763689]\n",
            "   ...\n",
            "   [ 0.10725483]\n",
            "   [ 0.1071065 ]\n",
            "   [ 0.10685929]]\n",
            "\n",
            "  [[-0.06298709]\n",
            "   [-0.07921833]\n",
            "   [-0.09483856]\n",
            "   ...\n",
            "   [-0.04833496]\n",
            "   [-0.05135514]\n",
            "   [-0.05409288]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.11198343]\n",
            "   [ 0.10901536]\n",
            "   [ 0.1075792 ]\n",
            "   ...\n",
            "   [-0.47358792]\n",
            "   [-0.4730374 ]\n",
            "   [-0.47263048]]\n",
            "\n",
            "  [[ 0.11560247]\n",
            "   [ 0.11415906]\n",
            "   [ 0.11257597]\n",
            "   ...\n",
            "   [-0.49502751]\n",
            "   [-0.49449205]\n",
            "   [-0.49409628]]\n",
            "\n",
            "  [[ 0.05637804]\n",
            "   [ 0.05491243]\n",
            "   [ 0.05231214]\n",
            "   ...\n",
            "   [ 0.07677849]\n",
            "   [ 0.07677849]\n",
            "   [ 0.07677849]]\n",
            "\n",
            "  [[ 0.10043184]\n",
            "   [ 0.10040712]\n",
            "   [ 0.10040712]\n",
            "   ...\n",
            "   [ 0.66772848]\n",
            "   [ 0.66772848]\n",
            "   [ 0.66772848]]\n",
            "\n",
            "  [[-0.1410445 ]\n",
            "   [-0.13661984]\n",
            "   [-0.13046032]\n",
            "   ...\n",
            "   [-0.04525898]\n",
            "   [-0.04525911]\n",
            "   [-0.04525922]]]\n",
            "\n",
            "\n",
            " [[[ 0.06353688]\n",
            "   [ 0.06353688]\n",
            "   [ 0.06353688]\n",
            "   ...\n",
            "   [-0.66397521]\n",
            "   [-0.66397521]\n",
            "   [-0.66397521]]\n",
            "\n",
            "  [[ 0.09911972]\n",
            "   [ 0.09911972]\n",
            "   [ 0.09911972]\n",
            "   ...\n",
            "   [-0.59764892]\n",
            "   [-0.5976722 ]\n",
            "   [-0.5976722 ]]\n",
            "\n",
            "  [[ 0.14058011]\n",
            "   [ 0.13977638]\n",
            "   [ 0.13847624]\n",
            "   ...\n",
            "   [ 0.10996762]\n",
            "   [ 0.10982579]\n",
            "   [ 0.1095894 ]]\n",
            "\n",
            "  [[ 0.13983706]\n",
            "   [ 0.13899654]\n",
            "   [ 0.13763689]\n",
            "   ...\n",
            "   [ 0.08147086]\n",
            "   [ 0.08129782]\n",
            "   [ 0.08107533]]\n",
            "\n",
            "  [[-0.06298709]\n",
            "   [-0.07921833]\n",
            "   [-0.09483856]\n",
            "   ...\n",
            "   [-0.0483288 ]\n",
            "   [-0.05134183]\n",
            "   [-0.05407271]]]\n",
            "\n",
            "\n",
            " [[[ 0.09941701]\n",
            "   [ 0.10370156]\n",
            "   [ 0.10786643]\n",
            "   ...\n",
            "   [ 0.06356081]\n",
            "   [ 0.07011929]\n",
            "   [ 0.07677351]]\n",
            "\n",
            "  [[ 0.10610393]\n",
            "   [ 0.11027118]\n",
            "   [ 0.11432203]\n",
            "   ...\n",
            "   [ 0.09087834]\n",
            "   [ 0.09723398]\n",
            "   [ 0.10370602]]\n",
            "\n",
            "  [[ 0.07124697]\n",
            "   [ 0.07124697]\n",
            "   [ 0.07124697]\n",
            "   ...\n",
            "   [ 0.24071485]\n",
            "   [ 0.24071485]\n",
            "   [ 0.24071485]]\n",
            "\n",
            "  [[ 0.08814552]\n",
            "   [ 0.0881208 ]\n",
            "   [ 0.0881208 ]\n",
            "   ...\n",
            "   [ 0.31745717]\n",
            "   [ 0.31743245]\n",
            "   [ 0.31743245]]\n",
            "\n",
            "  [[ 0.33473019]\n",
            "   [ 0.33393505]\n",
            "   [ 0.33178169]\n",
            "   ...\n",
            "   [ 0.39495961]\n",
            "   [ 0.40169361]\n",
            "   [ 0.40629709]]]]\n",
            "[[0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0]\n",
            " [0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfxkga2O-DYs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1fe3e086-c627-422e-fa02-6f962a1a2d31"
      },
      "source": [
        "inputs = keras.Input(shape = (Train_X.shape[1], Train_X.shape[2], Train_X.shape[3]), name=\"data\")\n",
        " \n",
        "x = layers.Conv2D(filters = 50, kernel_size = (2, 5),strides=(1, 2), activation = \"relu\") (inputs)\n",
        "x = layers.Conv2D(filters = 50, kernel_size = (1, 5),strides=(1, 2), activation = \"relu\") (x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "block_1_output = layers.MaxPooling2D(pool_size = (1,100)) (x)\n",
        "\n",
        "x = layers.Conv2D(filters = 50, kernel_size = (1,5),strides = (1, 2),padding = 'valid', activation='relu') (block_1_output)\n",
        "x = layers.Conv2D(filters = 50, kernel_size = (1,4),strides = (1, 2),padding = 'valid', activation='relu') (x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "block_3_output = layers.add([x, block_1_output])\n",
        "block_2_output = layers.MaxPooling2D(pool_size = (1,8)) (block_3_output)\n",
        "\n",
        "x = layers.Conv2D(filters = 50,kernel_size= (1, 1),strides = (1, 2), activation=\"relu\")(block_2_output)\n",
        "x = layers.BatchNormalization()(x)\n",
        "block_4_output = layers.add([x, block_1_output])\n",
        "x = layers.MaxPooling2D(pool_size = (1,1))(block_4_output)\n",
        "\n",
        "#x = layers.Conv2D(filters = 50,kernel_size= (1, 10),strides = (1, 2), activation=\"relu\")(x)\n",
        "\n",
        "#x = layers.AveragePooling2D(pool_size = (1,4))(x)\n",
        "#x = layers.Conv2D(filters = 25,kernel_size= (1, 10),strides = (1, 1), activation=\"relu\")(x)\n",
        "#x = layers.AveragePooling2D(pool_size = (1,4))(x)\n",
        "\n",
        "'''\n",
        "x = layers.Conv2D(filters = 21, kernel_size = (3, 100), activation='relu') (inputs)\n",
        "block_1_output = layers.MaxPooling2D(pool_size = (1,4)) (x)\n",
        " \n",
        "x = layers.Conv2D(filters = 11, kernel_size = (2, 200), activation=\"relu\", padding=\"same\")(block_1_output)\n",
        "x = layers.Conv2D(filters = 21, kernel_size = (2, 20), activation=\"relu\", padding=\"same\")(x)\n",
        "block_2_output = layers.add([x, block_1_output])\n",
        " \n",
        " \n",
        "x = layers.Conv2D(filters = 21,kernel_size= (1, 50), activation=\"relu\", padding=\"same\")(block_2_output)\n",
        "x = layers.Conv2D(filters = 21,kernel_size= (1, 70), activation=\"relu\", padding=\"same\")(x)\n",
        "block_3_output = layers.add([x, block_2_output])\n",
        " \n",
        " \n",
        "x = layers.Conv2D(filters = 11, kernel_size = (2, 20), activation=\"relu\", padding=\"same\")(block_3_output)\n",
        "x = layers.Conv2D(filters = 21, kernel_size = (2, 20), activation=\"relu\", padding=\"same\")(x)\n",
        " \n",
        "block_6_output = layers.add([x, block_3_output])\n",
        " \n",
        "x = layers.Conv2D(filters = 21,kernel_size= (1, 20), activation=\"relu\", padding=\"same\")(block_6_output)\n",
        "x = layers.Conv2D(filters = 21,kernel_size= (1, 20), activation=\"relu\", padding=\"same\")(x)\n",
        "block_7_output = layers.add([x, block_6_output])\n",
        " \n",
        "x = layers.MaxPooling2D(pool_size = (1,4)) (block_7_output) \n",
        " '''\n",
        "x = layers.Flatten()(x) \n",
        " \n",
        " \n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "#x = layers.BatchNormalization()(x)\n",
        "block_5_output = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(block_5_output)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "block_6_output = layers.add([x, block_5_output])\n",
        "x = layers.Dense(58, activation=\"relu\")(block_6_output)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(46, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(46, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(40, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "\n",
        "x = layers.Dense(28, activation=\"relu\")(x)\n",
        "x = layers.Dense(22, activation=\"relu\")(x)\n",
        "#block_4_output = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "block_7_output = layers.BatchNormalization()(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "#x = layers.Dense(16, activation=\"relu\")(block_4_output)\n",
        "#x = layers.Dense(16, activation=\"relu\")(x)\n",
        "block_8_output = layers.add([x, block_7_output])\n",
        "x = layers.Dense(16, activation=\"relu\")(block_7_output)\n",
        "\n",
        "x = layers.BatchNormalization()(x)\n",
        "#block_5_output = layers.add([x, block_4_output])\n",
        " \n",
        "#x = block_5_output\n",
        "#x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(Train_Y.shape[1], activation='softmax')(x)\n",
        " \n",
        " \n",
        "model = keras.Model(inputs, outputs, name=\"model\")\n",
        " \n",
        "model.compile(optimizer='adam', loss='CategoricalCrossentropy', metrics=['accuracy'])\n",
        " \n",
        "model.summary()\n",
        "plot_model(model, to_file='/content/drive/My Drive/Colab_Data/Fault_Detect/model.png',show_shapes=True)\n",
        "plt.imshow(mpimg.imread('/content/drive/My Drive/Colab_Data/Fault_Detect/model.png'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " data (InputLayer)              [(None, 5, 5000, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 4, 2498, 50)  550         ['data[0][0]']                   \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 4, 1247, 50)  12550       ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 4, 1247, 50)  200        ['conv2d_1[0][0]']               \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 4, 12, 50)    0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 4, 4, 50)     12550       ['max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 4, 1, 50)     10050       ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 4, 1, 50)    200         ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 4, 12, 50)    0           ['batch_normalization_1[0][0]',  \n",
            "                                                                  'max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 4, 1, 50)    0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 4, 1, 50)     2550        ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 4, 1, 50)    200         ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 4, 12, 50)    0           ['batch_normalization_2[0][0]',  \n",
            "                                                                  'max_pooling2d[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 4, 12, 50)   0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 2400)         0           ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 64)           153664      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 64)          256         ['dense[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 64)           4160        ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 64)          256         ['dense_1[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 64)           4160        ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 64)          256         ['dense_2[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 64)           4160        ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 64)          256         ['dense_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 64)           0           ['batch_normalization_6[0][0]',  \n",
            "                                                                  'batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 58)           3770        ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 58)          232         ['dense_4[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 52)           3068        ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 52)          208         ['dense_5[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 52)           2756        ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 52)          208         ['dense_6[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 52)           2756        ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 52)          208         ['dense_7[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 52)           2756        ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 52)          208         ['dense_8[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 46)           2438        ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 46)          184         ['dense_9[0][0]']                \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 52)           2444        ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 52)          208         ['dense_10[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 52)           2756        ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 52)          208         ['dense_11[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 52)           2756        ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 52)          208         ['dense_12[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 52)           2756        ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 52)          208         ['dense_13[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 46)           2438        ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 46)          184         ['dense_14[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 40)           1880        ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 40)          160         ['dense_15[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 32)           1312        ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 28)           924         ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 22)           638         ['dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 16)           368         ['dense_18[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 16)          64          ['dense_19[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 16)           272         ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 16)          64          ['dense_23[0][0]']               \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 9)            153         ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 244,811\n",
            "Trainable params: 242,723\n",
            "Non-trainable params: 2,088\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f22b223b450>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAAD8CAYAAAA7dIkaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2da2wj2ZXff7eKxbckUo8m1XqM1K/p6RljXu3x2Aliw0Y2ziSI98MmsQMkTuDAQNYJnE9ZO/mwyGOB7AbIC1isPUCceIPNeh3vLmIYThyvX0AGsOfhHbun3T3dPWp1q/WiRJEUX0WyWDcfWPdOUVJ3UyRHrXL3HyioWCxVlY5OnXvuuf9zjpBS8gj3hvGgHyAIeCSkHvBISD3gkZB6wCMh9YBHQuoBRy4kIcTHhRBvCyFuCCG+cNT37wfiKP0kIYQJXAP+MnAHeA34lJTyF0f2EH3gqDXpBeCGlHJJStkEvgZ84oif4dAIHfH9ZoAV3+c7wAf8JwghPgt8FiCRSDx//vz5fRdpNBqEQiGazSbhcBjHcXBdl1AohJQS13UxDAPDMGg2mxhGRxei0ShvvPHGtpRy6jAPfdRCui+klC8DLwNcvHhRvv766/vOuXz5MpFIhFKpxOTkJPV6nVarRSwWIxqNUq1WicVixONxVldXCYfDNBoNnnnmGYQQtw77TEctpFVgzvd51jt2KGSzWZrNJqOjo5imSSwWQwiBEALTNIlGowCEw2FmZmYAcByn74c+aiG9BpwVQizSEc4ngb9z2ItMTEz09EdLKYnH4wCEQv3/qUcqJCmlI4T4x8B3ABP4ipTycj/XyuVy1Ot1ms0m8/PzrKysMDc3x9raGq7rMjY2xubmJgDxeJyzZ8/2/dxHbpOklN8Gvj3odSqVCoZh4Lou5XKZRqOB4zg0m01M02R3d5eRkRHa7Ta2bQ90r2NnuHvFiRMn9L5hGGSzWUzTZHp6GiGEHtFc10UIMdC9Aikkv61Rn/3G2n8cQAiB4zh926XACml5eRnLskilUriuy8rKCvPz8xQKBVqtFtlslpWVFer1OtFolHQ6zdzc3P0vfgACO8F1XRfHcbhz547WmGKxiH+aFQqFGBsb0w5mvwikJgkhmJmZQQiBlBIhBKdOncIwDC0kwzCYnZ3Vxj0wLsAwUK/XEUJgWRaAFpIfjuNgGAamaWpBSin7HuUCJ6RyuUwul2N0dJRqtUqj0cA0TRYWFlheXsY0TSqVCrFYjFAoxPT0NJubm0QiEcbGxvq6ZyBtUqvVolAoIKXUQz10Rjb/K+fHICGhwGlSMpnk3Llz+hVTr5sQQo9eKjqgoI736y8FTpPi8bgWCqBtjoIQQhtpZYtc18V1XWKxWF/3DJwmAdy4cYOpqSl2d3dxXZdms8mZM2dYXl5mYWGB27dvAx0XwDRNwuEwu7u7PPnkk33dL3CaBJ3XaXd3V49YoVCIUqmEbdu0Wi1mZmYwTZN0Ok0kEiESiZBKpfq+XyA16amnntL7KgpZLBZ54okndDTy5MmTWJbFyMjIwPcLjJAKhYIOz0opsSyLdruNaZq8+eabXLhwgVarpc8vl8s64DYoAiOkpaUl4vG4NtKzs7Osra0xNzdHIpFgdXUVx3EQQuC6LolEgmq1SiKRGPjegRFSKBQiEolgGAaJRIKtrS0ajQZSSkzTBCCVStFsNrWD2Ww2Hy4hnT9/vsvP8XvPZ86c2Xe+mpbYtq3DKP0iMEKq1+tAR6Pq9TqWZWEYBkII2u02oVCIRqOB67qEw2EtxO3tbWZnZwe6d2CElMvliEQihEIhisUijuMwOTlJOBxmZ2eHTCbD1tYWrVYL0zS1kIZhvAPlJzWbTYrFIpZlEQ6HKRaLXd+rBUn/lGTQ0C0ESJNOnTql99V87fbt26TTadLpNNCZ10G3YA4KpRwWx1pIrVaLXC5HPB7Htm0dJ4pEIjiOg23bFAoFHMfBNE0sy8KyLEqlEvF4nHQ6zdra2sCv3LEX0q1btzh58iSVSoVWq4UQoktjdnZ2aDQahMNhIpEIExMT5PN5TNNESqm1bBAca5tkGAaWZVGr1TBNk0gkgmVZmjARCoUQQhCNRnUUEjpxJdM0WVlZ6VpV6RfHWpOi0Sjve9/7Dv17CwsL1Gq1gRclFY61kFzXZW1tjXQ6TaVSIZFIUCwWCYfDxONxyuWy9pGU86iEk81mh+JtQw+vmxDiK0KInBDiLd+xcSHEd4UQ172fae+4EEL8Z4/q93MhxHO+3/m0d/51IcSne33AQqGA67psbGzgui61Wo16vU673WZnZ4dKpUK1WqVUKlEqlQiFQliWhZSS3d3dw8rjQPRik/4b8PE9x74AfE9KeRb4nvcZ4K8CZ73ts8DvQUeowG/SIWy9APymEuy9oFZqVTjEcRxmZmaYnJwkFAoRi8WwLItkMqkjlqOjo0xNTWGaJvPz8z38eT1AhR7utQELwFu+z28D097+NPC2t/9lOhzIrvOATwFf9h3vOu9u2/PPPy/b7Xbfm+u6ci+A13v5m/1bvzYpI6Vc9/Y3gIy3fxDdb+Yex++LO3fuMDIyQq1WAzpUwKmpKYrFol54bDQaRCIREokEzWaTer3O4uJin3/afgxsuKWUUggxNAqvnzM5Pz+PEIJisUir1aLdbtNsNpmamsK2bVzXpd1u4zgO7XabRqNBIpHoCr4NA/36SZtCiGkA72fOO343ul/PNEAp5ctSyotSyotTU1Ok02lc19Wx6tHRUYQQhMNhotEoiUSCVCqlJ7/RaLRr7jYU9GmT/h3wBW//C8DvePt/DfjfgABeBF71jo8DN4G0t90ExnuxSa7ravuyd/Pbnr3b3cB7YZOEEH8IfASYFELcoTNK/Vvg60KIzwC3gL/lnf5t4CXgBlAD/oH3j9gRQvxrOpxJgH8lpdzp5Z8ohCCfz1OpVPSammEYtFotRkdH2djY0Eva8XicWq02tNi2wn2FJKX81F2++tgB50rgc3e5zleArxzq6Ty0Wi0ajYaet5VKJRzHIZlM6vmcZVnEYjEajUY/t7gnjrXHrRAOh0kkEjoCEA6HdWRS2SylYYOGag/CsReS67pdMSNAT1pd12V0dLTr/NHRUU3Y8pMpBsGxFpLjONy8eZPR0VEdKnFdl3g8zvj4OGtra4RCIWq1mo4lTUxMkMvlyGazJJPJoUQmj3WoxHVddnd3KZVK1Go1nUNSq9WQUmq/qVwu66Uk5TdBZ943DBxrTfJTjkdGRvRrpLRjfHycarVKJpMhkUhoOvLk5CSu6w7NPh17IWWzWf3Zdd0umk08HmdycpJSqaSdTADbtrFtm2QySbvdHvg5jrWQ9mJpaYlqtUo0GmVmZob19XVOnjzJ+vq6tkPb29tsbm4yPj5OPp+n3W4TDoep1+t9RwUCJSQ1XxsfH6dSqehUiWw2q4NuagI8MTGhY07j4+OaI9APAiWk6elpbWscxyGdTmvfCTr5JplMpjOV8PymiYmJLqZuPwiUkCYmJrSHDXRxj1zXZWlpiRdeeGHf70lv7e2hSZu4cuWKXneDzkg3PT3N+vo6oVCIzc1NotEo+XyeEydOkMvlsCyLdDrdN9vtWPtJB6HZbFKtVnFdF8uysG2bjY0NWq2WnuAq+F+vfl81CKAmLSwsaD4SdKYhamJbr9cZGxvDNE0d5z5x4oTmCPSLwAlpcnIS6GgUoJeNYrEYly9f5kMf+hBCCG3MgYGDcIETkkK9XufWrVucPn2amzdvEgqFGBkZ4e233yYUCpHJZGg2mxQKBS5cuDDQvQJnkxS2t7ffjRx6y92maRKPx3XqxMbGRt85bn4EVpMymYy2NyoSubW1xdRUpy6CYRicOXOmy371i8BqUiKR0A6jCpPcunVLr+AahjFQjpsfgdUk6PhM4XCYxcVFrl+/zrPPPsvNmzc100Qx4Z599tmB7hNYTQL0dMNxHE3qmp+f19oViUQG8o8UAq1Jp0+fBjpe9/nz5ymVSpimqY31+Pj4UEIlgdMkKSW1Wm2fZ20YBlevXtWaoyKXio5TLpf7vmcgNenOnTs6RULxuIUQPPnkk+RyOWzbJp1Os7GxoV+7UqnEc889d/+LH4DAaRJ0qku0221arRaO42jtGRkZ0ey2vQsAgywIBFKTYrFYV4EWKaXOYlpYWKDZbGJZVtdy08mTJ/u+X6A0SUqpV0ZUxqTjOPzoRz8iHo/TbDZpNpvUajWi0WjX1m9qKQRQk9555x0sy2JxcZGVlRW9WHD79m3N5Q6FQmxtbXUVfRkEvXAm54QQPxBC/EIIcVkI8Xnv+JHxJv0IhUJ6Vq/mbP5kHDWHGx8fH4qPBNyfekOHzvectz9CpwTiBeB36Kbf/La3/xLd9Juf+Og3S97PtLefvh/1xg/XdWWj0di31ev1rs+tVktKKeX29vbRUG9kh/a37u2XhRBX6FD5PkGHkgPwVeCHwG94x3/fe6AfCyFSHtHrI8B3FeVGCPFdOoTVP7zX/R3H6SoopR5cedWNRgPDMPTPUCik09trtRoTExP3+xPvi0PZJCHEAvAs8BPeI97kXjqgbdusrq6STCap1WqaAhiJRPTaWygUolqt6kwAteI7yIjmR8+jmxAiCfwx8E+llF0EaU9rhmIA5B46oFoyUpFIpUGAHuoVPVCFaJVdOlJWiRDCoiOgP5BS/ol3eFMIMS2lXD8Eb/Ije47/8H73jsViLCwsHPjda6+9xvvf/35gf8rWsAQEvY1uAvgvwBUp5b/3ffVNQI1Qnwb+l+/43/NGuReBkvdafgf4FSFE2hsJf8U7dr/761FsZ2eHYrGIbdvs7Oxw6dIl1tfX2draolQqkc/nabVaOs40LPSiSX8B+LvAJSHEm96xf84R8iYB2u02a2trRKNRZmdn2d7e5sMf/rC2V8oWqeKcw0Qvo9v/ozOcH4Qj400qu6O0KhwO67x/dWyY0Ug/AuFxq2D/448/ro8tLi7yyiuv6CUkhbvVTxoEx15IalnIsixarRZSSk0k3djYIJfLaTaJ8r5d1yWZTA6lTgkEREhbW1vE43Hq9boWUjgc5qWXXuL27dsYhkG1WtXzNkWzeWiEZJqmZtTGYrEuZm25XNZGWo2C6tUcJlX52AtJjWYH4fvf/z4f/ehHD/xuGKxbhWMvJDWzV4U3TdMklUqxs7NDLpfjnXfe0SES13WZmJjQ2d7DMuCBCbq5rkuxWNQLAPV6nYsXL5LP56nValSrVer1+tCSk/049pqkYBgGY2NjOm4Ui8UoFAq6JJmUUqfFDxvHXkjqlVEFNxUSiQSXLl3ipZde6irv6v+dvfO5fnHshdRoNNjc3CQWi2HbNu12W9OOk8kkN2/e1HElKSXhcFhXdm82m0xPTw/8DMdeSI7jUK1WcRynK03LdV2ef/55VldXabfblMtlIpGIdhHq9frQ5nDHXkiKFKoCbX4/SeW/Kf6k8pMU3WZ8fHwoz3DshRQOh7tSJ6B7aI/H45RKJWKxmF4gWFpaIpvN+uP0v9ycyYMMb6lUYmtri0wmw+bmJoZhsLm5SSqVIhaLce3aNa1VkUiEYrE4ECXw2AvpIAghutK5FBcgk8ngui6nT59GSkmlUiGdTv9yF5m6GxT1WAhBOp3WGuM4jk6BF0IQj8c14XQQBFJIyWRSp2n5K9v89Kc/5bnnnusa1VQwbhAEUkgKly9fJhwOMzExoSvcbG5uks/ndUGqZrPZFazrB4GZux0ElbAcjUa1A+kPzJXLZV2fchAEWpNU4yllmyqVin4VVarpQ8mZVCMadKdyRaNRrl27plO5lHCEEDr966FJ5QJYXl7W5RQB7YXPzc2xurpKo9EglUqxvr6uV3+FEH3Vh4OA2iTFqlW1J9WrlU6n9UTXn0oqpRyoPEcgNSkcDnPy5Mmuxi1CCMrlclfaxMLCghbWIFTlwAlJTWrh3XQuhUuXLvGBD3xAn6eE99CmckHHT7Isi0wmQ6FQIJVKsbW1RT6f1zEmy7K6egr0g14IE1EhxKtCiJ95dMB/6R1fFEL8xKP9/ZEQIuwdj3ifb3jfL/iu9UXv+NtCiL8y0JNDV2NOf8BNjWzRaHQoGQG90AEFkPT2LToErheBrwOf9I5/CfhH3v6vA1/y9j8J/JG3fwH4GRABFoF3APMwdMC9KJfLcnd3V1arVbm7uytXVlZkqVTSx3d3d2WpVBqYDnhfTfKuXfE+Wt4mgY8C3/COfxX4VW//E95nvO8/5tF3PgF8TUrZkFLepMM62Z+ffv/n0WU2/GlblmVx/fp1TarwN3ap1+tdaRaHRa8kLhN4AzgD/C4dLShKKVWfVT+1T9P+ZKdbaQmY8I7/2HfZnuiAB+H27dtdfpIKlZw7d46NjY19aROGYVCv1/tO6erJT5JStqWUz9Bhp70A7O8FPSTspQMeBJU2ocpwwLtLTv51N/+UZJDpyaFGNyllUQjxA+CDQEoIEfK0yV8SUdEB7wghQsAYkGdI3ZQBIpGITuPyo1gsau2TUvL4449r4QzSTrGX0W1KCJHy9mN02thfAX4A/Jp32l46oKIJ/hrwfc9gfhP4pDf6LdKpj/vqYR9YLUyqmrgKUkquXLmi2bkqe0nKTv1c5Vv1g140aRr4qmeXDODrUspvCSF+AXxNCPFvgD+nw6vE+/nfhRA3gB06IxxSystCiK8DvwAc4HNSyoHG52vXrmFZFtlslkKhwPT0NIVCgZ2dHc2bbDabPP3004Pcpic64M/pcLf3Hl/igNFJSmkDf/Mu1/ot4LcO/5gHI5VK0Wg0iMfj5PP5A9O3hkEwDbTHrcjsQggWFhbI5/OMj49zN4PfLwInJCmlLuiiaDnKOF+5ckXP3QDdRVmlfvVb9SZwQgJYX1/XlZIV9U8IwVNPPUU+n9/nJ6klqOeff76v+wUynnTixAndR9K/7J1IJHTcSNmjYVCXA6lJsViMc+fO7Tuey+W6ipUP0kLRj8AJyR8f2tra0nO0drvN1atXddFy1UwBBidOBE5IfqiyrrOzs+TzeWZnZymXy7rrhBAC27YHFlIgbZLC2NiYnvGrOJI/hUvsKTbVLwKtSX7q8qlTp1hbW+PEiRNkMpl7/NbhETghlctlbNvWS0Uq8mhZFpcvX2Z8fFy3UCyXyzqMq7p09YPACUnRlGOxmG4TpCa9Z8+exXEc8vk8MzMz7Ozs6Aadv9Qkrr0Ih8OMjo6yvr6OaZqa+ue6LidOnKBUKnWRTNXPQSpyBU5IypH0l+FQuH37NvPz89pW+bsLDoLACUllR0JHKOFwmHA4TLPZZHl5mWQySb1e11okpRw4qztwQvKjVCphGAbZbJZyucz09DS1Wk037xRC0Gg0Hm4hjY+P6z5KyldSJHjlMw2jG07ghOQP6PvZ/o899hjvvPPOPh/JH0rpl2AaOCGVy2V2d3eJxWK6GZ7qtbSyskImk2F3d5dUKsXm5iaRSATbthkZGenbyQyckPxdAm3b1onJrVaL8+fPY9s21WpVLy85jkOz2Xy4QiWqaNTOzg4jIyNdBV7C4bD2kQDdoSIajQ40hwuckFQJstHR0X0LjlevXuX8+fPaViUSiYcjlWsv/H/0tWvX9GhWr9d1ymmpVNKaI6UcuGtg4ITkh6L5pdNpisUiU1NTNBoNms0mUkq9ADAoAi2kdDqtjbIa7VR3Zdd1h9YHN3BC8vGmutbX4vE4ly5dOpAjMGiXrsAJaXt7m1KpRDqdplQqad6kao5Xq9V0Xe7V1VWdOjExMdH3ouVhKnGZQog/F0J8y/v8QOiAtm0Tj8fJ5XKUy2VdtbRarfLEE09oQSmbpGyUouj0g8No0ufpsElUGdDfBv6DlPJrQogvAZ+h00H5M0BBSnlGCPFJ77y/LYS4QIc88SRwEvgzIcS5w5ImxsbGgHfX1fydutTcTWUKqPw3Veilb/TCGaTDJfoeHQrgt+jwKLeBkPf9B4HvePvfAT7o7Ye88wTwReCLvmvq8/rhTLquKx3H6dpeffXVfcccxxmYM9mrJv1H4J/RqTMJHXrfe0IH7BXFYpHNzU0sy+rquGXbNsvLy5ormU6nB26m0AuJ668DOSnlGwPdqUcIIT4rhHhdCPH61tbWXc9TZAjHcZidnaVUKulGeP7l72F0V+61ptvfEEK8BETp2KT/xHtEB5RSvgy8DHDx4sW7Eh2TyaQWUigUIpFIUK/XEUJw8uRJbYuGgsO8m3RKIH7L2/+fdPO4f93b/xzdPO6ve/tP0s3jXqIPHne73ZatVktvzWZT77/yyisHftdsNmWz2XzPbdJB+A0eAB1wc3OTQqHA5OQkhUJBz/pHR0cxDAPbtllbW2NmZobl5WVdymx6errvqsqHZd/+EK+ApnxAdEDHcchkMmxsbOgeAEIIms0mFy5cYGdnR8/ZpC9KMMgcLnAe98TEBK7rHpjKpYJtsVgMIQSzs7P6u0HsU+CE5O+mvNeLvnTpEi+++CLRaLTLgRyUNBE4ISkUCgU2NjZ0YeCpqSlSqRStVoulpSU9ZxsbG+Oxxx4b6F6Bpd74G3RmMhny+TxSvptWql7DhzqepGyP9OLbqiy1aZosLi5qo+033v0icEJqtVqabqMq26jJrlp382d5G4ZBrVbTzJN+EDgh5XI5dnZ2mJqa0sO98pPS6bQutTg3N8fNmzcxDINWq0U2m+1bSIGzSVJK5ubm2Nra0rW3FWHi7NmzbG1t6c7L/t8Z5LULnCapXLe7pXJlMhndudRPvRlESIHTpHA4TCwW6wqiKbbblStXuhxMtQQej8cHWhQInCYplEqlrjIb8/PzevZ/69YtTNOk1WoxOjo6UK4bBFCT/EgkErTbbRYXF9ne3t5ni2A4hYIDq0npdFpriIohWZZFOBzmzJkz+ryH0k+ybZtWq9XVEQc6a2qKouy6rg6RhEIh3das39pugRPS9vY2xWJRL22r8GwymWRubg7XdVldXdVlglSKaTab7VtIgbNJhmEwNzdHoVBASql74DqOw/z8PNvb29Trda1Nioj6UIVKMpkM7Xb7rqlcMzMzusPyQb5UPwickBTB3bZtarWannbEYjGuXLlCOp3WTRWUR/7QpnKVy2VyuZx2Ki3LYn5+HimlzoNrNpskk8mHN5XLNE1NRZ6ZmaFQKFCpVDQ1WZ0zjAbmgdWkVCrV1Z10bm5O11J66P2kR6lcPWBvKpeahjxK5fLhbqlcUkqmpqYepXLBo1SunqA86Gq1yu7urp6jjY2Nsby8rMO6kUiERqNBNBoduGlw4ISkUKvVdAkgZcSnp6eRUlIqlfS6WzKZHFhIPVkzIcSyEOKSEOJNIcTr3rEH0lFZIRwOk0qlsCyLycnJrhZByWRSRzCH0hGnR8rNMjC559iRd1SWskMDVFu73e7arl27tu9Yu93W5z8I6s2RdFTei2KxSLlcZmRkhGq1qkMlsViMW7dukc1mKZVKpFIpPW1RYdzJycm+/tBehSSB/yuEkMCXZYeNdiQdlfeiUqngui47Ozt60RE6lJxnnnkG27apVCqMjY1Rr9c1PXmQbji9CukvSilXhRAngO8KIa76v5RSSk+AA0Pehw6oOkgclMoF3UWDVSoXMBBFuSchSSlXvZ85IcSf0iFvHUlH5b1QM/peU7lg8MWAXti3CSHEiNqn0wn5LY6oo/IBz6Nrb9+6dYvV1VWuX79OpVLRr+CNGzdYWVnh+vXrbGxsHAmJKwP8qXeTEPA/pJT/RwjxGkfYUXkvVDpEq9UilUpRrVaZnJxESqn5AbZtD5YJ4KGXEopLwL5CjVLKPEfYUXkvYrEYqVSKZrNJIpHQeSbwbnl8fxOqQRA4j1sZ6ng8rqmB0DHom5ubJBKJfUvaj1K5fKlc29vbB6ZyNRoNJicn+07lCpyQ/KlcKk4khKBSqfD000/vS+VyXVene/WLwAlpbyqXn6L8XqVyBU5IKq7tdxQVVFcuBWWzBl0MCJyQFPypXLZt63QtfypXrVZjfHz8vU/lOq7wp3LNzMwcmMqlFi4HRWA1SaVytdvtfalcMzMzXWmngyKwQlJN7qDjO0WjUa5evcqpU6eIRCJDa1sGARZSsVjUaRNSSjKZDLFYjEajobssq7SJu3Wt6BWBtUmqeXC73dZl7y3L6iJ2KXLpoAisJo2Ojuq2QEIIxsbGdC+Tubm5ob1qEGAhqfokgCZz/fznP2dubk4XBx6U5K7vNfAVHhD8Nml3d5ezZ88+SuXaC38q17lz5w5M5VLnDYrAapI/lcs0zfc0lSuwmqTq/itfKRQKsbS0hGEYukuXaZoDFbxTCKwm+W1SrVbj1KlTOjnn5s2bmKaJ4ziMjIywsLAw0L0Cq0nQmeW3223Onj3L9vY2rVary08ahhZBgDUplUqRSCR0LCmbzWJZFqFQqIuaPAzDHThNajab1Go13YPbP6JduXJFnye8rlyGYVCpVKhWq33fM3CalMvluspw+NNLlU1S6aW3bt3Swf/x8fG+c94Cp0lCCObm5tje3tYxJeU4njlzhq2tLZ024Q+TPJRpE2fPnt0XDsnn80xPT5PNZjEMY2hpE4HTJMVsU9lHrVYL27Zpt9u6U6Bt27ronaIERqPR/u85xOc/UpRKJe0ntdttZmZmdAGXO3fu6OE/Go3qFZZ+0SsdMCWE+IYQ4qoQ4ooQ4oMPmg5oGIZOm3jsscc0WcKfwqWmKQOjRzrgV4F/6O2HgRQPiA6o0G63ZaPR6Npu376975ht212/x3tBBxRCjAF/Cfj7nlCbQFMI8UDogNVqVde7dRxH+0ihUIi33nqLdDpNq9UiHA7rlC6VPtHva9eLTVoEtoD/KoR4mk5n5c/zgOiAOzs7VCoVRkZGKJVKOjybSCQ4ffo0ruvq5MCtrS1M00RKSSqV6ltIvdikEPAc8HtSymeBKp3XS8PTmqHRAeU9OipblsXJkyfZ3d1FCKG7cqkSQTs7O9Tr9a6UrkFtUy+adAe4I6X8iff5G3SE9EDogCpt4qD00rW1NWZnZ3XahD+laxD00uV9A1gRQjzuHfoYnQp/D4QOqOJIrVaLQqFAqVTSXvb169exbZvNzU2KxSL5fJ5isai1rV/0+pv/BPgD0amUvESH4mfwAOmAlUpF87QNwyASieg1/3K5rLMow+Fw3/xthV7Zt9qfOmwAAAFfSURBVG8CFw/46oHRAdXStm3bZLNZTeBS/biBgVO4FMQw16eGDSFEGXh7CJeapFPNGeAxKeWhKG/HfVrytpTyIA0+FIQQrw9yncBNcB8EHgmpBxx3Ib18HK5zrA33ccFx16RjgUdC6gHHVkhCiI+LTn+TG0KIL9zn3KHkCN8Vhw1AHcUGmMA7wCk6Qb6fARfucf4yA+YI32s7rpr0AnBDSrnkBfm+RieYdxh8gk4wEO/nr/qO/74XqPwxnYYQ0wddQOG4CqmnAJ0PKkf4DS9oB4cPCt4Vx31a0ive0xzh46pJPfU4UZC+HGGgK0cYoMeg4F1xXIX0GnBWdDp/hem09fjmQScOMUf47njQI9k9RqyXgGt0Rrl/cY/zTtEZ/X4GXFbn0unf9D3gOvBnwLh3XAC/6133EnDxfs/yaFrSA47r63as8EhIPeCRkHrAIyH1gEdC6gGPhNQDHgmpB/x/GpNUgBMz264AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del(Data_X)\n",
        "del(Data_Y)"
      ],
      "metadata": {
        "id": "ZnnTG7-yD-78"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Data_X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "RFXiJxTxEwgs",
        "outputId": "14cbf89b-d65a-463c-eebb-10ba95138d97"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ad6a4062474f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Data_X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W3lFxV88qlI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "483ab619-d3a2-4308-bf54-d09a74cbffd9"
      },
      "source": [
        "'''from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "model_optimizer = KerasClassifier(build_fn = model, verbose=0)\n",
        "batch_size = [10, 20, 40, 60]#, 80, 100, 200, 400, 600]\n",
        "epochs = [10, 50, 100]#, 150, 200]\n",
        "param_grid = dict(batch_size = batch_size, epochs=epochs)\n",
        "grid = GridSearchCV(estimator = model_optimizer, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "Train_X_sample = Train_X[:int(len(Train_X)/5000)]\n",
        "Train_Y_sample = Train_Y[:int(len(Train_Y)/5000)]\n",
        "grid_result = grid.fit(Train_X_sample, Train_Y_sample)\n",
        "\n",
        "#summarize results\n",
        "print(\"Best: %f using %s\" %(grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "'''"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from sklearn.model_selection import GridSearchCV\\nfrom keras.wrappers.scikit_learn import KerasClassifier\\nmodel_optimizer = KerasClassifier(build_fn = model, verbose=0)\\nbatch_size = [10, 20, 40, 60]#, 80, 100, 200, 400, 600]\\nepochs = [10, 50, 100]#, 150, 200]\\nparam_grid = dict(batch_size = batch_size, epochs=epochs)\\ngrid = GridSearchCV(estimator = model_optimizer, param_grid=param_grid, n_jobs=-1, cv=3)\\nTrain_X_sample = Train_X[:int(len(Train_X)/5000)]\\nTrain_Y_sample = Train_Y[:int(len(Train_Y)/5000)]\\ngrid_result = grid.fit(Train_X_sample, Train_Y_sample)\\n\\n#summarize results\\nprint(\"Best: %f using %s\" %(grid_result.best_score_, grid_result.best_params_))\\nmeans = grid_result.cv_results_[\\'mean_test_score\\']\\nstds = grid_result.cv_results_[\\'std_test_score\\']\\nparams = grid_result.cv_results_[\\'params\\']\\nfor mean, stdev, param in zip(means, stds, params):\\n  print(\"%f (%f) with: %r\" % (mean, stdev, param))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2ZHnfqh-E6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea655fb2-f431-4b48-80ca-9267ff0f1e20"
      },
      "source": [
        "history = model.fit(Train_X, Train_Y, epochs=100,batch_size = 50, verbose=2)#, validation_data=(Vald_X,Vald_Y))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "322/322 - 24s - loss: 1.7358 - accuracy: 0.3483 - 24s/epoch - 75ms/step\n",
            "Epoch 2/100\n",
            "322/322 - 10s - loss: 0.6950 - accuracy: 0.6445 - 10s/epoch - 32ms/step\n",
            "Epoch 3/100\n",
            "322/322 - 10s - loss: 0.5570 - accuracy: 0.7037 - 10s/epoch - 32ms/step\n",
            "Epoch 4/100\n",
            "322/322 - 10s - loss: 0.4794 - accuracy: 0.7489 - 10s/epoch - 32ms/step\n",
            "Epoch 5/100\n",
            "322/322 - 11s - loss: 0.4396 - accuracy: 0.7840 - 11s/epoch - 34ms/step\n",
            "Epoch 6/100\n",
            "322/322 - 10s - loss: 0.3710 - accuracy: 0.8281 - 10s/epoch - 32ms/step\n",
            "Epoch 7/100\n",
            "322/322 - 10s - loss: 0.3199 - accuracy: 0.8497 - 10s/epoch - 31ms/step\n",
            "Epoch 8/100\n",
            "322/322 - 10s - loss: 0.2997 - accuracy: 0.8590 - 10s/epoch - 31ms/step\n",
            "Epoch 9/100\n",
            "322/322 - 10s - loss: 0.3369 - accuracy: 0.8432 - 10s/epoch - 32ms/step\n",
            "Epoch 10/100\n",
            "322/322 - 10s - loss: 0.2939 - accuracy: 0.8597 - 10s/epoch - 31ms/step\n",
            "Epoch 11/100\n",
            "322/322 - 10s - loss: 0.2734 - accuracy: 0.8640 - 10s/epoch - 31ms/step\n",
            "Epoch 12/100\n",
            "322/322 - 10s - loss: 0.2809 - accuracy: 0.8625 - 10s/epoch - 31ms/step\n",
            "Epoch 13/100\n",
            "322/322 - 10s - loss: 0.2854 - accuracy: 0.8616 - 10s/epoch - 31ms/step\n",
            "Epoch 14/100\n",
            "322/322 - 10s - loss: 0.2896 - accuracy: 0.8607 - 10s/epoch - 31ms/step\n",
            "Epoch 15/100\n",
            "322/322 - 11s - loss: 0.2890 - accuracy: 0.8642 - 11s/epoch - 33ms/step\n",
            "Epoch 16/100\n",
            "322/322 - 10s - loss: 0.2769 - accuracy: 0.8703 - 10s/epoch - 31ms/step\n",
            "Epoch 17/100\n",
            "322/322 - 10s - loss: 0.2860 - accuracy: 0.8647 - 10s/epoch - 31ms/step\n",
            "Epoch 18/100\n",
            "322/322 - 10s - loss: 0.2482 - accuracy: 0.8824 - 10s/epoch - 31ms/step\n",
            "Epoch 19/100\n",
            "322/322 - 10s - loss: 0.2394 - accuracy: 0.8840 - 10s/epoch - 31ms/step\n",
            "Epoch 20/100\n",
            "322/322 - 10s - loss: 0.2239 - accuracy: 0.8926 - 10s/epoch - 32ms/step\n",
            "Epoch 21/100\n",
            "322/322 - 10s - loss: 0.2609 - accuracy: 0.8735 - 10s/epoch - 31ms/step\n",
            "Epoch 22/100\n",
            "322/322 - 10s - loss: 0.2543 - accuracy: 0.8746 - 10s/epoch - 32ms/step\n",
            "Epoch 23/100\n",
            "322/322 - 10s - loss: 0.2134 - accuracy: 0.8955 - 10s/epoch - 32ms/step\n",
            "Epoch 24/100\n",
            "322/322 - 10s - loss: 0.2639 - accuracy: 0.8711 - 10s/epoch - 32ms/step\n",
            "Epoch 25/100\n",
            "322/322 - 10s - loss: 0.2406 - accuracy: 0.8856 - 10s/epoch - 33ms/step\n",
            "Epoch 26/100\n",
            "322/322 - 10s - loss: 0.2062 - accuracy: 0.9005 - 10s/epoch - 32ms/step\n",
            "Epoch 27/100\n",
            "322/322 - 10s - loss: 0.2602 - accuracy: 0.8815 - 10s/epoch - 31ms/step\n",
            "Epoch 28/100\n",
            "322/322 - 10s - loss: 0.2127 - accuracy: 0.8990 - 10s/epoch - 31ms/step\n",
            "Epoch 29/100\n",
            "322/322 - 10s - loss: 0.2268 - accuracy: 0.8854 - 10s/epoch - 31ms/step\n",
            "Epoch 30/100\n",
            "322/322 - 10s - loss: 0.1974 - accuracy: 0.9040 - 10s/epoch - 32ms/step\n",
            "Epoch 31/100\n",
            "322/322 - 10s - loss: 0.1846 - accuracy: 0.9066 - 10s/epoch - 32ms/step\n",
            "Epoch 32/100\n",
            "322/322 - 10s - loss: 0.1809 - accuracy: 0.9093 - 10s/epoch - 32ms/step\n",
            "Epoch 33/100\n",
            "322/322 - 10s - loss: 0.2444 - accuracy: 0.8845 - 10s/epoch - 32ms/step\n",
            "Epoch 34/100\n",
            "322/322 - 10s - loss: 0.2042 - accuracy: 0.8991 - 10s/epoch - 32ms/step\n",
            "Epoch 35/100\n",
            "322/322 - 11s - loss: 0.2037 - accuracy: 0.8979 - 11s/epoch - 35ms/step\n",
            "Epoch 36/100\n",
            "322/322 - 10s - loss: 0.1911 - accuracy: 0.9039 - 10s/epoch - 33ms/step\n",
            "Epoch 37/100\n",
            "322/322 - 10s - loss: 0.2241 - accuracy: 0.8879 - 10s/epoch - 32ms/step\n",
            "Epoch 38/100\n",
            "322/322 - 10s - loss: 0.2113 - accuracy: 0.8956 - 10s/epoch - 32ms/step\n",
            "Epoch 39/100\n",
            "322/322 - 10s - loss: 0.2262 - accuracy: 0.8890 - 10s/epoch - 33ms/step\n",
            "Epoch 40/100\n",
            "322/322 - 10s - loss: 0.1881 - accuracy: 0.9039 - 10s/epoch - 32ms/step\n",
            "Epoch 41/100\n",
            "322/322 - 10s - loss: 0.1732 - accuracy: 0.9084 - 10s/epoch - 32ms/step\n",
            "Epoch 42/100\n",
            "322/322 - 10s - loss: 0.2148 - accuracy: 0.8932 - 10s/epoch - 32ms/step\n",
            "Epoch 43/100\n",
            "322/322 - 10s - loss: 0.1828 - accuracy: 0.9095 - 10s/epoch - 32ms/step\n",
            "Epoch 44/100\n",
            "322/322 - 10s - loss: 0.1767 - accuracy: 0.9127 - 10s/epoch - 32ms/step\n",
            "Epoch 45/100\n",
            "322/322 - 10s - loss: 0.2048 - accuracy: 0.8997 - 10s/epoch - 31ms/step\n",
            "Epoch 46/100\n",
            "322/322 - 10s - loss: 0.1636 - accuracy: 0.9171 - 10s/epoch - 31ms/step\n",
            "Epoch 47/100\n",
            "322/322 - 10s - loss: 0.1715 - accuracy: 0.9145 - 10s/epoch - 31ms/step\n",
            "Epoch 48/100\n",
            "322/322 - 10s - loss: 0.1763 - accuracy: 0.9112 - 10s/epoch - 31ms/step\n",
            "Epoch 49/100\n",
            "322/322 - 10s - loss: 0.1522 - accuracy: 0.9224 - 10s/epoch - 31ms/step\n",
            "Epoch 50/100\n",
            "322/322 - 10s - loss: 0.1872 - accuracy: 0.9062 - 10s/epoch - 31ms/step\n",
            "Epoch 51/100\n",
            "322/322 - 10s - loss: 0.1708 - accuracy: 0.9177 - 10s/epoch - 32ms/step\n",
            "Epoch 52/100\n",
            "322/322 - 10s - loss: 0.1447 - accuracy: 0.9270 - 10s/epoch - 31ms/step\n",
            "Epoch 53/100\n",
            "322/322 - 10s - loss: 0.1690 - accuracy: 0.9198 - 10s/epoch - 32ms/step\n",
            "Epoch 54/100\n",
            "322/322 - 11s - loss: 0.1860 - accuracy: 0.9103 - 11s/epoch - 33ms/step\n",
            "Epoch 55/100\n",
            "322/322 - 11s - loss: 0.1508 - accuracy: 0.9250 - 11s/epoch - 33ms/step\n",
            "Epoch 56/100\n",
            "322/322 - 10s - loss: 0.1519 - accuracy: 0.9266 - 10s/epoch - 32ms/step\n",
            "Epoch 57/100\n",
            "322/322 - 10s - loss: 0.1387 - accuracy: 0.9264 - 10s/epoch - 32ms/step\n",
            "Epoch 58/100\n",
            "322/322 - 10s - loss: 0.1515 - accuracy: 0.9205 - 10s/epoch - 32ms/step\n",
            "Epoch 59/100\n",
            "322/322 - 10s - loss: 0.1636 - accuracy: 0.9209 - 10s/epoch - 32ms/step\n",
            "Epoch 60/100\n",
            "322/322 - 10s - loss: 0.1347 - accuracy: 0.9312 - 10s/epoch - 33ms/step\n",
            "Epoch 61/100\n",
            "322/322 - 10s - loss: 0.1836 - accuracy: 0.9076 - 10s/epoch - 32ms/step\n",
            "Epoch 62/100\n",
            "322/322 - 10s - loss: 0.1381 - accuracy: 0.9310 - 10s/epoch - 32ms/step\n",
            "Epoch 63/100\n",
            "322/322 - 10s - loss: 0.1393 - accuracy: 0.9277 - 10s/epoch - 32ms/step\n",
            "Epoch 64/100\n",
            "322/322 - 10s - loss: 0.1314 - accuracy: 0.9320 - 10s/epoch - 32ms/step\n",
            "Epoch 65/100\n",
            "322/322 - 10s - loss: 0.1654 - accuracy: 0.9194 - 10s/epoch - 32ms/step\n",
            "Epoch 66/100\n",
            "322/322 - 10s - loss: 0.1564 - accuracy: 0.9258 - 10s/epoch - 33ms/step\n",
            "Epoch 67/100\n",
            "322/322 - 10s - loss: 0.1253 - accuracy: 0.9338 - 10s/epoch - 32ms/step\n",
            "Epoch 68/100\n",
            "322/322 - 10s - loss: 0.1461 - accuracy: 0.9178 - 10s/epoch - 32ms/step\n",
            "Epoch 69/100\n",
            "322/322 - 10s - loss: 0.1933 - accuracy: 0.8998 - 10s/epoch - 32ms/step\n",
            "Epoch 70/100\n",
            "322/322 - 10s - loss: 0.1730 - accuracy: 0.9136 - 10s/epoch - 32ms/step\n",
            "Epoch 71/100\n",
            "322/322 - 10s - loss: 0.1448 - accuracy: 0.9206 - 10s/epoch - 32ms/step\n",
            "Epoch 72/100\n",
            "322/322 - 10s - loss: 0.1567 - accuracy: 0.9167 - 10s/epoch - 32ms/step\n",
            "Epoch 73/100\n",
            "322/322 - 10s - loss: 0.1305 - accuracy: 0.9229 - 10s/epoch - 32ms/step\n",
            "Epoch 74/100\n",
            "322/322 - 11s - loss: 0.1329 - accuracy: 0.9220 - 11s/epoch - 34ms/step\n",
            "Epoch 75/100\n",
            "322/322 - 10s - loss: 0.1506 - accuracy: 0.9188 - 10s/epoch - 32ms/step\n",
            "Epoch 76/100\n",
            "322/322 - 10s - loss: 0.1881 - accuracy: 0.9065 - 10s/epoch - 32ms/step\n",
            "Epoch 77/100\n",
            "322/322 - 10s - loss: 0.1550 - accuracy: 0.9222 - 10s/epoch - 32ms/step\n",
            "Epoch 78/100\n",
            "322/322 - 10s - loss: 0.1298 - accuracy: 0.9290 - 10s/epoch - 32ms/step\n",
            "Epoch 79/100\n",
            "322/322 - 10s - loss: 0.1487 - accuracy: 0.9229 - 10s/epoch - 32ms/step\n",
            "Epoch 80/100\n",
            "322/322 - 10s - loss: 0.1215 - accuracy: 0.9329 - 10s/epoch - 32ms/step\n",
            "Epoch 81/100\n",
            "322/322 - 10s - loss: 0.1499 - accuracy: 0.9243 - 10s/epoch - 32ms/step\n",
            "Epoch 82/100\n",
            "322/322 - 10s - loss: 0.1470 - accuracy: 0.9211 - 10s/epoch - 32ms/step\n",
            "Epoch 83/100\n",
            "322/322 - 10s - loss: 0.1410 - accuracy: 0.9281 - 10s/epoch - 32ms/step\n",
            "Epoch 84/100\n",
            "322/322 - 10s - loss: 0.1284 - accuracy: 0.9320 - 10s/epoch - 32ms/step\n",
            "Epoch 85/100\n",
            "322/322 - 10s - loss: 0.1191 - accuracy: 0.9354 - 10s/epoch - 32ms/step\n",
            "Epoch 86/100\n",
            "322/322 - 10s - loss: 0.1272 - accuracy: 0.9300 - 10s/epoch - 32ms/step\n",
            "Epoch 87/100\n",
            "322/322 - 10s - loss: 0.1429 - accuracy: 0.9215 - 10s/epoch - 32ms/step\n",
            "Epoch 88/100\n",
            "322/322 - 10s - loss: 0.1375 - accuracy: 0.9211 - 10s/epoch - 32ms/step\n",
            "Epoch 89/100\n",
            "322/322 - 10s - loss: 0.1644 - accuracy: 0.9136 - 10s/epoch - 32ms/step\n",
            "Epoch 90/100\n",
            "322/322 - 10s - loss: 0.1885 - accuracy: 0.9074 - 10s/epoch - 32ms/step\n",
            "Epoch 91/100\n",
            "322/322 - 10s - loss: 0.1430 - accuracy: 0.9230 - 10s/epoch - 32ms/step\n",
            "Epoch 92/100\n",
            "322/322 - 10s - loss: 0.1309 - accuracy: 0.9213 - 10s/epoch - 32ms/step\n",
            "Epoch 93/100\n",
            "322/322 - 10s - loss: 0.1213 - accuracy: 0.9288 - 10s/epoch - 32ms/step\n",
            "Epoch 94/100\n",
            "322/322 - 11s - loss: 0.1162 - accuracy: 0.9293 - 11s/epoch - 34ms/step\n",
            "Epoch 95/100\n",
            "322/322 - 10s - loss: 0.1220 - accuracy: 0.9301 - 10s/epoch - 32ms/step\n",
            "Epoch 96/100\n",
            "322/322 - 10s - loss: 0.1343 - accuracy: 0.9261 - 10s/epoch - 32ms/step\n",
            "Epoch 97/100\n",
            "322/322 - 10s - loss: 0.1075 - accuracy: 0.9384 - 10s/epoch - 32ms/step\n",
            "Epoch 98/100\n",
            "322/322 - 10s - loss: 0.1137 - accuracy: 0.9332 - 10s/epoch - 32ms/step\n",
            "Epoch 99/100\n",
            "322/322 - 10s - loss: 0.1050 - accuracy: 0.9382 - 10s/epoch - 32ms/step\n",
            "Epoch 100/100\n",
            "322/322 - 10s - loss: 0.1188 - accuracy: 0.9335 - 10s/epoch - 32ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0F_DkMZ-KVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e11818f-b512-4e66-82ee-8a52d634d66a"
      },
      "source": [
        "#Test_X,Test_Y = prepare_test_data()\n",
        "\n",
        "#Test_Y_1 = to_categorical(Test_Y)\n",
        "Train_Y_1 = to_categorical(Train_Y)\n",
        "print(Test_X.shape)\n",
        "print(Test_Y.shape)\n",
        "print(to_categorical(Test_Y).shape)\n",
        "print(Train_X.shape)\n",
        "print(Train_Y.shape)\n",
        "print(to_categorical(Train_Y).shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6886, 5, 5000, 1)\n",
            "(6886, 9)\n",
            "(6886, 9, 2)\n",
            "(16064, 5, 5000, 1)\n",
            "(16064, 9)\n",
            "(16064, 9, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgjRnDLQ-S8b"
      },
      "source": [
        "from numpy import argmax\n",
        "from numpy import random\n",
        "print ('Training   ', model.metrics_names,' = ',model.evaluate(Train_X, Train_Y, verbose=0))\n",
        "#print ('Validation ', model.metrics_names,' = ',model.evaluate(Vald_X,  Vald_Y, verbose=0))\n",
        "print ('Testing    ', model.metrics_names,' = ',model.evaluate(Test_X,  Test_Y, verbose=0))\n",
        "\n",
        "print ('true (Test)     \\n',argmax(Test_Y,axis=1))\n",
        "print ('predicted (Test)\\n',argmax(model.predict(Test_X), axis=1))\n",
        "\n",
        "print ('true (Train)     \\n',argmax(Train_Y[:10],axis=1))    \n",
        "print ('predicted (Train)\\n', argmax(model.predict(Train_X[:10]), axis=1))     #make random check: Test_X*0.0090, random.shuffle(Test_X), Test_X*random.random()*0.05            \n",
        "#print (model.summary())\n",
        " \n",
        " \n",
        "numpy.savetxt('/content/drive/My Drive/Colab_Data/Fault_Detect/Test_Data_Predict.csv',argmax(model.predict(Test_X), axis=1),delimiter=',')\n",
        "numpy.savetxt('/content/drive/My Drive/Colab_Data/Fault_Detect/Test_Data_True.csv',argmax(Test_Y,axis=1),delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh9bju8u-W5-"
      },
      "source": [
        "#Plot the model\n",
        " \n",
        "plot_model(model, to_file='model.png', show_shapes = True, show_layer_names = True)\n",
        " \n",
        "print(history.history.keys())\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        " \n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMxOw2un-hDZ"
      },
      "source": [
        "\n",
        "model.save('/content/drive/My Drive/Colab_Data/Fault_Detect/my_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}