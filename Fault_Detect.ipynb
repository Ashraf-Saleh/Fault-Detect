{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashraf-Saleh/Fault-Detect/blob/Master/Fault_Detect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaIp7ke4k58l",
        "outputId": "09092b9f-5df8-4640-81b6-7bc045a3e700"
      },
      "source": [
        "!pip install pydot\n",
        "!apt install graphviz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot) (3.0.9)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "graphviz is already the newest version (2.40.1-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9PFzkDY9Spa",
        "outputId": "25676b40-f0c6-439f-a804-5bb2ba4aebb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "import numpy\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import keras \n",
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense, Flatten, Dropout, Activation, BatchNormalization,Concatenate, merge\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import model_from_json\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import random, os\n",
        "import multiprocessing as mp\n",
        "from queue import Empty\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6101e06429a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'merge' from 'keras.layers' (/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4l8tGQJ9v91"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpO0tJE1908T"
      },
      "source": [
        "Data_X = numpy.load(\"/content/drive/My Drive/Colab_Data/Fault_Detect/Train_X_Sample.npy\",None,allow_pickle=True)\n",
        "Data_Y = numpy.load(\"/content/drive/My Drive/Colab_Data/Fault_Detect/Train_Y_Sample.npy\",None,allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyuU4RWd95V2"
      },
      "source": [
        "#Train_X = Data_X\n",
        "#Train_Y = Data_Y\n",
        "Train_X = Data_X[:int(len(Data_X)*0.7)]\n",
        "Train_Y = Data_Y[:int(len(Data_Y)*0.7)]\n",
        "#Vald_X = Data_X[int(len(Data_X)*0.4):int(len(Data_X)*0.7)]\n",
        "#Vald_Y = Data_Y[int(len(Data_Y)*0.4):int(len(Data_Y)*0.7)]\n",
        "Test_X = Data_X[int(len(Data_X)*0.7):]\n",
        "Test_Y = Data_Y[int(len(Data_Y)*0.7):]\n",
        " \n",
        "print(Data_X.shape)\n",
        "print(Train_X.shape)\n",
        "#print(Vald_X.shape)\n",
        "print(Test_X.shape)\n",
        "\n",
        "print(Data_Y.shape)\n",
        "print(Train_Y.shape)\n",
        "#print(Vald_Y.shape)\n",
        "print(Test_Y.shape)\n",
        "\n",
        "print(Data_X.shape)\n",
        "print(Data_Y.shape)\n",
        "print(Data_X[:10])\n",
        "print(Data_Y[:10].astype('int'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfxkga2O-DYs"
      },
      "source": [
        "inputs = keras.Input(shape = (Train_X.shape[1], Train_X.shape[2], Train_X.shape[3]), name=\"data\")\n",
        " \n",
        "x = layers.Conv2D(filters = 50, kernel_size = (2, 5),strides=(1, 2), activation = \"relu\") (inputs)\n",
        "x = layers.Conv2D(filters = 50, kernel_size = (1, 5),strides=(1, 2), activation = \"relu\") (x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "block_1_output = layers.MaxPooling2D(pool_size = (1,100)) (x)\n",
        "\n",
        "x = layers.Conv2D(filters = 50, kernel_size = (1,5),strides = (1, 2),padding = 'valid', activation='relu') (block_1_output)\n",
        "x = layers.Conv2D(filters = 50, kernel_size = (1,4),strides = (1, 2),padding = 'valid', activation='relu') (x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "block_3_output = layers.add([x, block_1_output])\n",
        "block_2_output = layers.MaxPooling2D(pool_size = (1,8)) (block_3_output)\n",
        "\n",
        "x = layers.Conv2D(filters = 50,kernel_size= (1, 1),strides = (1, 2), activation=\"relu\")(block_2_output)\n",
        "x = layers.BatchNormalization()(x)\n",
        "block_4_output = layers.add([x, block_1_output])\n",
        "x = layers.MaxPooling2D(pool_size = (1,1))(block_4_output)\n",
        "\n",
        "#x = layers.Conv2D(filters = 50,kernel_size= (1, 10),strides = (1, 2), activation=\"relu\")(x)\n",
        "\n",
        "#x = layers.AveragePooling2D(pool_size = (1,4))(x)\n",
        "#x = layers.Conv2D(filters = 25,kernel_size= (1, 10),strides = (1, 1), activation=\"relu\")(x)\n",
        "#x = layers.AveragePooling2D(pool_size = (1,4))(x)\n",
        "\n",
        "'''\n",
        "x = layers.Conv2D(filters = 21, kernel_size = (3, 100), activation='relu') (inputs)\n",
        "block_1_output = layers.MaxPooling2D(pool_size = (1,4)) (x)\n",
        " \n",
        "x = layers.Conv2D(filters = 11, kernel_size = (2, 200), activation=\"relu\", padding=\"same\")(block_1_output)\n",
        "x = layers.Conv2D(filters = 21, kernel_size = (2, 20), activation=\"relu\", padding=\"same\")(x)\n",
        "block_2_output = layers.add([x, block_1_output])\n",
        " \n",
        " \n",
        "x = layers.Conv2D(filters = 21,kernel_size= (1, 50), activation=\"relu\", padding=\"same\")(block_2_output)\n",
        "x = layers.Conv2D(filters = 21,kernel_size= (1, 70), activation=\"relu\", padding=\"same\")(x)\n",
        "block_3_output = layers.add([x, block_2_output])\n",
        " \n",
        " \n",
        "x = layers.Conv2D(filters = 11, kernel_size = (2, 20), activation=\"relu\", padding=\"same\")(block_3_output)\n",
        "x = layers.Conv2D(filters = 21, kernel_size = (2, 20), activation=\"relu\", padding=\"same\")(x)\n",
        " \n",
        "block_6_output = layers.add([x, block_3_output])\n",
        " \n",
        "x = layers.Conv2D(filters = 21,kernel_size= (1, 20), activation=\"relu\", padding=\"same\")(block_6_output)\n",
        "x = layers.Conv2D(filters = 21,kernel_size= (1, 20), activation=\"relu\", padding=\"same\")(x)\n",
        "block_7_output = layers.add([x, block_6_output])\n",
        " \n",
        "x = layers.MaxPooling2D(pool_size = (1,4)) (block_7_output) \n",
        " '''\n",
        "x = layers.Flatten()(x) \n",
        " \n",
        " \n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "#x = layers.BatchNormalization()(x)\n",
        "block_5_output = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(block_5_output)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "block_6_output = layers.add([x, block_5_output])\n",
        "x = layers.Dense(58, activation=\"relu\")(block_6_output)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(46, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(52, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(46, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(40, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "\n",
        "x = layers.Dense(28, activation=\"relu\")(x)\n",
        "x = layers.Dense(22, activation=\"relu\")(x)\n",
        "#block_4_output = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "block_7_output = layers.BatchNormalization()(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "#x = layers.Dense(16, activation=\"relu\")(block_4_output)\n",
        "#x = layers.Dense(16, activation=\"relu\")(x)\n",
        "block_8_output = layers.add([x, block_7_output])\n",
        "x = layers.Dense(16, activation=\"relu\")(block_7_output)\n",
        "\n",
        "x = layers.BatchNormalization()(x)\n",
        "#block_5_output = layers.add([x, block_4_output])\n",
        " \n",
        "#x = block_5_output\n",
        "#x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(Train_Y.shape[1], activation='softmax')(x)\n",
        " \n",
        " \n",
        "model = keras.Model(inputs, outputs, name=\"model\")\n",
        " \n",
        "model.compile(optimizer='adam', loss='CategoricalCrossentropy', metrics=['accuracy'])\n",
        " \n",
        "model.summary()\n",
        "plot_model(model, to_file='/content/drive/My Drive/Colab_Data/Fault_Detect/model.png',show_shapes=True)\n",
        "plt.imshow(mpimg.imread('/content/drive/My Drive/Colab_Data/Fault_Detect/model.png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W3lFxV88qlI"
      },
      "source": [
        "'''from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "model_optimizer = KerasClassifier(build_fn = model, verbose=0)\n",
        "batch_size = [10, 20, 40, 60]#, 80, 100, 200, 400, 600]\n",
        "epochs = [10, 50, 100]#, 150, 200]\n",
        "param_grid = dict(batch_size = batch_size, epochs=epochs)\n",
        "grid = GridSearchCV(estimator = model_optimizer, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "Train_X_sample = Train_X[:int(len(Train_X)/5000)]\n",
        "Train_Y_sample = Train_Y[:int(len(Train_Y)/5000)]\n",
        "grid_result = grid.fit(Train_X_sample, Train_Y_sample)\n",
        "\n",
        "#summarize results\n",
        "print(\"Best: %f using %s\" %(grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2ZHnfqh-E6C"
      },
      "source": [
        "history = model.fit(Train_X, Train_Y, epochs=100,batch_size = 50, verbose=2)#, validation_data=(Vald_X,Vald_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0F_DkMZ-KVe"
      },
      "source": [
        "#Test_X,Test_Y = prepare_test_data()\n",
        "\n",
        "#Test_Y_1 = to_categorical(Test_Y)\n",
        "Train_Y_1 = to_categorical(Train_Y)\n",
        "print(Test_X.shape)\n",
        "print(Test_Y.shape)\n",
        "print(to_categorical(Test_Y).shape)\n",
        "print(Train_X.shape)\n",
        "print(Train_Y.shape)\n",
        "print(to_categorical(Train_Y).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgjRnDLQ-S8b"
      },
      "source": [
        "from numpy import argmax\n",
        "from numpy import random\n",
        "print ('Training   ', model.metrics_names,' = ',model.evaluate(Train_X, Train_Y, verbose=0))\n",
        "#print ('Validation ', model.metrics_names,' = ',model.evaluate(Vald_X,  Vald_Y, verbose=0))\n",
        "print ('Testing    ', model.metrics_names,' = ',model.evaluate(Test_X,  Test_Y, verbose=0))\n",
        "\n",
        "print ('true (Test)     \\n',argmax(Test_Y,axis=1))\n",
        "print ('predicted (Test)\\n',argmax(model.predict(Test_X), axis=1))\n",
        "\n",
        "print ('true (Train)     \\n',argmax(Train_Y[:10],axis=1))    \n",
        "print ('predicted (Train)\\n', argmax(model.predict(Train_X[:10]), axis=1))     #make random check: Test_X*0.0090, random.shuffle(Test_X), Test_X*random.random()*0.05            \n",
        "#print (model.summary())\n",
        " \n",
        " \n",
        "numpy.savetxt('/content/drive/My Drive/Colab_Data/Fault_Detect/Test_Data_Predict.csv',argmax(model.predict(Test_X), axis=1),delimiter=',')\n",
        "numpy.savetxt('/content/drive/My Drive/Colab_Data/Fault_Detect/Test_Data_True.csv',argmax(Test_Y,axis=1),delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh9bju8u-W5-"
      },
      "source": [
        "#Plot the model\n",
        " \n",
        "plot_model(model, to_file='model.png', show_shapes = True, show_layer_names = True)\n",
        " \n",
        "print(history.history.keys())\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        " \n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMxOw2un-hDZ"
      },
      "source": [
        "\n",
        "model.save('/content/drive/My Drive/Colab_Data/Fault_Detect/my_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}